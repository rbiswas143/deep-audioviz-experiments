{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import dataset\n",
    "import importlib\n",
    "import nets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import sklearn\n",
    "import keras\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Reload any packages here\n",
    "\n",
    "# importlib.reload(dataset)\n",
    "\n",
    "# Analysis config\n",
    "\n",
    "save_dir = 'cached/fma_small_mfcc_conv_m10_fps5_test'\n",
    "mfcc_save_path = os.path.join(save_dir, 'mfcc.npy')\n",
    "tracks_save_path = os.path.join(save_dir, 'tracks')\n",
    "data_prep_params_save_path = os.path.join(save_dir, 'data_prep_params')\n",
    "training_params_save_path = os.path.join(save_dir, 'training_params')\n",
    "encoder_save_path = os.path.join(save_dir, 'encoder')\n",
    "model_save_path = os.path.join(save_dir, 'model')\n",
    "\n",
    "analysis_mode = 'ae' # options: ae, genre\n",
    "analysis_data_type = 'test' # options: teast, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCCs data loaded. Data size 72000\n",
      "Tracks data loaded. Data size (10, 52)\n",
      "Data prep params loaded (10, 44100, 5, 20, 20, 'cached/fma_small_mfcc_conv_m10_fps5_test')\n",
      "Training params loaded (-1.8013664173677022, 58.824029222879325, 0.8, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "\n",
    "# Load mfccs, prams and norms\n",
    "x = np.load(mfcc_save_path)# mfccs for all tracks\n",
    "print('MFCCs data loaded. Data size', x.size)\n",
    "\n",
    "tracks = pd.read_pickle(tracks_save_path)\n",
    "print('Tracks data loaded. Data size', tracks.shape)\n",
    "\n",
    "with open(data_prep_params_save_path, 'rb') as pf: # dataset processing params\n",
    "    data_prep_params = pickle.load(pf)\n",
    "num_tracks, sr, fps, num_mfcc, num_segments_per_track, save_dir = data_prep_params\n",
    "# params_dict = {k: v for k, v in list(locals().items()) if len(list(filter(lambda x: x is v, params))) == 1}\n",
    "print('Data prep params loaded', data_prep_params)\n",
    "\n",
    "with open(training_params_save_path, 'rb') as nf:\n",
    "    training_params = pickle.load(nf)\n",
    "mean, std, data_split_ratio, num_net_scale_downs = training_params\n",
    "print('Training params loaded', training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reshaped (200, 20, 18, 1)\n",
      "Data padded (200, 24, 24, 1)\n",
      "New mfcc dimensions (24, 24)\n",
      "Training shape (160, 24, 24, 1)\n",
      "Test shape (40, 24, 24, 1)\n",
      "Data normalized DescribeResult(nobs=23040, minmax=(-5.380438857628132, 4.67300144232842), mean=0.05002037655904822, variance=1.0362581139814935, skewness=-0.27678801801817365, kurtosis=12.901607840377752)\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "\n",
    "# Shape for training\n",
    "num_mfcc_frames = int(x.size / (num_tracks * num_segments_per_track * num_mfcc))\n",
    "x = x.reshape(num_tracks * num_segments_per_track, num_mfcc, num_mfcc_frames, 1)\n",
    "print('Data reshaped', x.shape)\n",
    "\n",
    "# Padding mfcc data to make the training/test dimensions divisible by 2 ** num_net_scale_downs\n",
    "divisor = 2 ** num_net_scale_downs\n",
    "\n",
    "num_pad_mfcc_frames = (0 if num_mfcc_frames % divisor == 0 else(int(num_mfcc_frames / divisor) + 1) * divisor - num_mfcc_frames)\n",
    "x_pad_frames = np.zeros((num_tracks * num_segments_per_track, num_mfcc, num_pad_mfcc_frames, 1))\n",
    "x = np.concatenate((x, x_pad_frames), axis=2)\n",
    "num_mfcc_frames_new = x.shape[2]\n",
    "\n",
    "num_pad_mfcc = (0 if num_mfcc % divisor == 0 else (int(num_mfcc / divisor) + 1) * divisor - num_mfcc)\n",
    "x_pad_mfcc = np.zeros((num_tracks * num_segments_per_track, num_pad_mfcc, num_mfcc_frames_new, 1))\n",
    "x = np.concatenate((x, x_pad_mfcc), axis=1)\n",
    "num_mfcc_new = x.shape[1]\n",
    "print('Data padded', x.shape)\n",
    "print('New mfcc dimensions', (num_mfcc_new, num_mfcc_frames_new))\n",
    "\n",
    "# Split\n",
    "x_train, x_test = dataset.split_data(x, data_split_ratio)\n",
    "print('Training shape', x_train.shape)\n",
    "print('Test shape', x_test.shape)\n",
    "\n",
    "# Choose data\n",
    "x_analysis = x_test if analysis_data_type == 'test' else x_train\n",
    "\n",
    "# Normalize\n",
    "x_analysis = (x_analysis - mean) / std\n",
    "print('Data normalized', scipy.stats.describe(x_analysis.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tracks shape (8, 52)\n",
      "Test tracks shape (2, 52)\n",
      "Analysis data type: test (40, 2)\n"
     ]
    }
   ],
   "source": [
    "# Process tracks metadata\n",
    "\n",
    "track_index = tracks.index\n",
    "\n",
    "num_segements_train = x_train.shape[0]\n",
    "num_tracks_train = int(num_segements_train/num_segments_per_track)\n",
    "tracks_train_idx = track_index[:num_tracks_train]\n",
    "tracks_train = tracks.loc[tracks_train_idx, :]\n",
    "print('Training tracks shape', tracks_train.shape)\n",
    "\n",
    "num_segements_test = x_test.shape[0]\n",
    "num_tracks_test = int(num_segements_test/num_segments_per_track)\n",
    "tracks_test_idx = tracks.index[num_tracks_train:]\n",
    "tracks_test = tracks.loc[tracks_test_idx, :]\n",
    "print('Test tracks shape', tracks_test.shape)\n",
    "\n",
    "tracks_analysis = tracks_test if analysis_data_type == 'test' else tracks_train\n",
    "num_segements_analysis = num_segements_test if analysis_data_type == 'test' else num_segements_train\n",
    "num_tracks_analysis = num_tracks_test if analysis_data_type == 'test' else num_tracks_train\n",
    "tracks_analysis_idx = tracks_analysis.index\n",
    "print('Analysis data type:', analysis_data_type, (num_segements_analysis, num_tracks_analysis))\n",
    "\n",
    "# Separate a few genres\n",
    "hiphop = tracks_analysis[tracks_analysis['track', 'genre_top'] == 'Hip-Hop']\n",
    "folk = tracks_analysis[tracks_analysis['track', 'genre_top'] == 'Folk']\n",
    "pop = tracks_analysis[tracks_analysis['track', 'genre_top'] == 'Pop']\n",
    "electronic = tracks_analysis[tracks_analysis['track', 'genre_top'] == 'Electronic']\n",
    "instrumental = tracks_analysis[tracks_analysis['track', 'genre_top'] == 'Instrumental']\n",
    "experimental = tracks_analysis[tracks_analysis['track', 'genre_top'] == 'Experimental']\n",
    "international = tracks_analysis[tracks_analysis['track', 'genre_top'] == 'International']\n",
    "rock = tracks_analysis[tracks_analysis['track', 'genre_top'] == 'Rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded (15,)\n",
      "Encoder loaded (8,)\n",
      "Prediction shape (40, 3, 3, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbiswas/.virtualenvs/deep-audioviz3/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "\n",
    "# Load net and weights\n",
    "model = keras.models.load_model(model_save_path)\n",
    "print('Model loaded', (len(model.layers),))\n",
    "encoder = keras.models.load_model(encoder_save_path)\n",
    "print('Encoder loaded', (len(encoder.layers),))\n",
    "# model, encoder = classifiers.genre_classifier_conv(inpdimx, inpdimy, 2)\n",
    "# model.load_weights(net_save_path)\n",
    "\n",
    "# Predict\n",
    "y = encoder.predict(x_analysis)\n",
    "print('Prediction shape', y.shape)\n",
    "# print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened encodings by segment shape (40, 9)\n",
      "Flattened encodings by track shape (2, 20, 9)\n",
      "Raw encoding length: 9\n"
     ]
    }
   ],
   "source": [
    "if type(y) is list:\n",
    "    print('Multi layer output')\n",
    "    raise Exception('Merge layers here')\n",
    "\n",
    "# Reshape encodings\n",
    "y_segment = y.reshape(num_tracks_analysis * num_segments_per_track, -1)\n",
    "print('Flattened encodings by segment shape', y_segment.shape)\n",
    "y = y.reshape(num_tracks_analysis, num_segments_per_track, -1)\n",
    "print('Flattened encodings by track shape', y.shape)\n",
    "raw_encoding_length = int(y_segment.size / (num_segements_analysis))\n",
    "print('Raw encoding length:', raw_encoding_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encodings(y_curr, tracks_curr_idx):\n",
    "    encodings = {}\n",
    "    for i, idx in enumerate(tracks_curr_idx):\n",
    "        encodings[idx] = y_curr[i, :, :]\n",
    "    print('Generated encoding shape', list(encodings.values())[0].shape)\n",
    "    return encodings\n",
    "\n",
    "encodings_map = generate_encodings(y, tracks_analysis_idx)\n",
    "print('Encodings map generated', (len(encodings_map),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform encodings to principal components\n",
    "\n",
    "def transform_encodings_with_pca(y_segment_curr=y_segment, \n",
    "                                 reduced_dim=raw_encoding_length,\n",
    "                                 scale='feature',\n",
    "                                _range=(0, 100)):\n",
    "\n",
    "    pca = PCA(n_components=reduced_dim)\n",
    "    pca.fit(y_segment_curr)\n",
    "    y_segment_pca = pca.transform(y_segment_curr)\n",
    "    print('Variance retained: {}%'.format(pca.explained_variance_ratio_.sum()*100))\n",
    "    print('Data transformed', pd.Series(y_segment_pca.reshape(-1)).describe())\n",
    "\n",
    "    if scale == 'feature':\n",
    "        scaler = sklearn.preprocessing.MinMaxScaler(_range)\n",
    "        scaler.fit(y_segment_pca.reshape(-1, 1))\n",
    "        y_segment_pca = scaler.transform(y_segment_pca)\n",
    "        print('Data scaled', pd.Series(y_segment_pca.reshape(-1)).describe())\n",
    "    elif scale == 'all':\n",
    "        y_segment_flat = y_segment_pca.reshape(-1, 1)\n",
    "        scaler = sklearn.preprocessing.MinMaxScaler(_range)\n",
    "        scaler.fit(y_segment_flat)\n",
    "        y_segment_pca = scaler.transform(y_segment_flat).reshape(y_segment_pca.shape[0], -1)\n",
    "        print('Data scaled', pd.Series(y_segment_pca.reshape(-1)).describe())\n",
    "        \n",
    "    if True:\n",
    "        print('Variance by components')\n",
    "        print(pca.explained_variance_ratio_.cumsum())\n",
    "        \n",
    "    return y_segment_pca\n",
    "    \n",
    "# y_segment_pca = transform_encodings_with_pca()\n",
    "y_segment_pca = transform_encodings_with_pca()\n",
    "print('Transformed encodings by segment shape', y_segment_pca.shape)\n",
    "y_pca = y_segment_pca.reshape(num_tracks_analysis, num_segments_per_track, -1)\n",
    "print('Transformed encodings by track shape', y_pca.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best cluster\n",
    "y_segment_curr = y_segment_pca\n",
    "\n",
    "_cluster_range = range( 1, 20 )\n",
    "_cluster_errors = []\n",
    "\n",
    "for _num_clusters in _cluster_range:\n",
    "  _clusters = KMeans( _num_clusters )\n",
    "  _clusters.fit( y_segment_pca )\n",
    "  _cluster_errors.append( _clusters.inertia_ )\n",
    "\n",
    "_clusters_df = pd.DataFrame( { \"num_clusters\":_cluster_range, \"cluster_errors\": _cluster_errors } )\n",
    "print(_clusters_df)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot( _clusters_df.num_clusters, _clusters_df.cluster_errors, marker = \"o\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality by using K means\n",
    "\n",
    "# TODO: Scale to a range\n",
    "def transform_encodings_with_kmeans(y_segment_curr=y_segment, reduced_dim=4):\n",
    "    kmeans = KMeans(n_clusters=reduced_dim)\n",
    "    kmeans.fit(y_segment_curr)\n",
    "    y_segment_kmeans = kmeans.transform(y_segment_curr)\n",
    "    print('Data transformed', pd.Series(y_segment_kmeans.reshape(-1)).describe())\n",
    "    print('Score', kmeans.score(y_segment_curr))\n",
    "    y_segment_kmeans = 1 / (1 + y_segment_kmeans)\n",
    "    print('Data similarity', pd.Series(y_segment_kmeans.reshape(-1)).describe())\n",
    "    return y_segment_kmeans\n",
    "\n",
    "y_segment_kmeans = transform_encodings_with_kmeans()\n",
    "print('Tansformed encodings by segment shape', y_segment_kmeans.shape)\n",
    "y_kmeans = y_segment_kmeans.reshape(num_tracks_analysis, num_segments_per_track, -1)\n",
    "print('Reduced encodings by track shape', y_kmeans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis: Encoding stats\n",
    "y_curr = y# options: y, y_pca, y_kmeans\n",
    "y_segment_curr = y_segment # options: y_segment, y_segment_pca, y_segment_kmeans\n",
    "\n",
    "# Overall stats\n",
    "if True:\n",
    "    print('Num tracks:', y_curr.shape[0])\n",
    "    print('Num segments:', y_segment_curr.shape[0])\n",
    "    print('Distribution across entire encoding')\n",
    "    print(pd.Series(y_curr.reshape(-1)).describe())\n",
    "\n",
    "# Stats for all encoding components\n",
    "if True:\n",
    "    pylab.rcParams['figure.figsize'] = (14,8)\n",
    "    _max_outputs = 200\n",
    "    _idx = np.arange(y_segment_curr.shape[1])\n",
    "    if _idx.size > _max_outputs:\n",
    "        np.random.shuffle(_idx)\n",
    "        _idx = _idx[:_max_outputs]\n",
    "    _y_segment_curr = y_segment_curr[:, _idx]\n",
    "\n",
    "    print('Plotting stats for {} components'.format(_idx.size))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title('Mean')\n",
    "    plt.bar(np.arange(_idx.size), _y_segment_curr[:, _idx].mean(axis=0))\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title('Min')\n",
    "    plt.bar(np.arange(_idx.size), _y_segment_curr[:, _idx].min(axis=0))\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.title('Max')\n",
    "    plt.bar(np.arange(_idx.size), _y_segment_curr[:, _idx].max(axis=0))\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title('Variance')\n",
    "    plt.bar(np.arange(_idx.size), _y_segment_curr[:, _idx].var(axis=0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Percentiles combined\n",
    "if True:\n",
    "    print('Plotting percentiles {} components'.format(_idx.size))\n",
    "    pylab.rcParams['figure.figsize'] = (14,12)\n",
    "    _percentiles = [10, 30, 50 ,70, 90, 100]\n",
    "    for i, _p in enumerate(_percentiles):\n",
    "        plt.subplot(3, 2, i+1)\n",
    "        plt.title('{} Percentile'.format(_p))\n",
    "        plt.bar(np.arange(_idx.size), np.percentile(y_segment_curr[:, _idx], _p, axis=0))\n",
    "    plt.show()\n",
    "\n",
    "# Percentiles separated\n",
    "if True:\n",
    "    pylab.rcParams['figure.figsize'] = (14,12)\n",
    "    _max_outputs = 20\n",
    "    _percentiles = range(0,100,10)\n",
    "    _idx = np.arange(y_segment_curr.shape[1])\n",
    "    if _idx.size > _max_outputs:\n",
    "        np.random.shuffle(_idx)\n",
    "        _idx = _idx[:_max_outputs]\n",
    "    y_segment_curr = y_segment_curr[:, _idx]\n",
    "    for i, _index in enumerate(_idx):\n",
    "        plt.subplot(int(_idx.size/2), int(_idx.size/2)+1, i+1)\n",
    "        plt.title('Component {}'.format(_index))\n",
    "        plt.bar(_percentiles, list(map(lambda _p: np.percentile(y_segment_curr[:, _index].reshape(-1), _p), _percentiles)))\n",
    "#         plt.ylim(0,100)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# print(print(pd.Series(y_segment[:,_index]).describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 2 dimensions of the encodings for multiple segments in scatter plots\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (20, 4)\n",
    "\n",
    "_y_curr = y # options: y, y_red\n",
    "_y_segment_curr = y_segment # options: y_segment, y_segment_red\n",
    "_encoding_length = int(_y_segment_curr.size / num_segements_analysis)\n",
    "\n",
    "_encodings_map = generate_encodings(_y_curr, tracks_analysis_idx)\n",
    "\n",
    "_sample = np.arange(int(num_segments_per_track))\n",
    "np.random.shuffle(_sample)\n",
    "\n",
    "for _genre in [folk, electronic]:\n",
    "    for i,_idx in enumerate(_genre.index):\n",
    "        if i==0:\n",
    "            print('Plotting {0}'.format(_genre['track', 'genre_top'][_idx]))\n",
    "        _toplot = range(0,_encoding_length ,2)\n",
    "        for i, _dim in enumerate(_toplot):\n",
    "            _dim1 = _dim\n",
    "            _dim2 = _dim+1\n",
    "            if _dim2 >= _encoding_length:\n",
    "                continue\n",
    "            _encoded = _encodings_map[_idx]\n",
    "            _x = np.transpose(_encoded[_sample, _dim1])\n",
    "            _y = np.transpose(_encoded[_sample, _dim2])\n",
    "            plt.subplot(1,len(_toplot), i+1)\n",
    "            plt.xlabel('Dim {0}'.format(_dim1))\n",
    "            plt.ylabel('Dim {0}'.format(_dim2))\n",
    "            plt.scatter(_x, _y, marker='^', c='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize encodings for random segment from multiple genres\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (20, 12)\n",
    "\n",
    "_y_curr = y # options: y, y_red\n",
    "_y_segment_curr = y_segment # options: y_segment, y_segment_red\n",
    "_encoding_length = int(_y_segment_curr.size / num_segements_analysis)\n",
    "\n",
    "_encodings_map = generate_encodings(_y_curr, tracks_analysis_idx)\n",
    "\n",
    "for i, _genre in enumerate([hiphop, folk, electronic, rock, instrumental, international, experimental]):\n",
    "# for i, _genre in enumerate([folk, electronic]):\n",
    "    _track_idx = _genre.index[np.random.randint(_genre.shape[0])]\n",
    "    _encoding = _encodings_map[_track_idx]\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.title(_genre['track', 'genre_top'][_track_idx])\n",
    "    plt.bar(np.arange(_encoding.shape[1]), _encoding[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the encodings of multiple segmets of the same track\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (20, 3)\n",
    "\n",
    "_y_curr = y # options: y, y_red\n",
    "_y_segment_curr = y_segment # options: y_segment, y_segment_red\n",
    "_encoding_length = int(y_segment_curr.size / num_segements_analysis)\n",
    "\n",
    "_encodings_map = generate_encodings(_y_curr, tracks_analysis_idx)\n",
    "_segments_to_visulize = 10\n",
    "\n",
    "# for i, genre in enumerate([hiphop, folk, electronic, rock, instrumental, international, experimental]):\n",
    "for i, _genre in enumerate([folk, electronic]):\n",
    "    _track_idx = _genre.index[np.random.randint(_genre.shape[0])]\n",
    "    print('Plotting {0}'.format(_genre['track', 'genre_top'][_track_idx]))\n",
    "    _encoding = _encodings_map[_track_idx]\n",
    "    for i in range(_segments_to_visulize):\n",
    "        plt.subplot(1, _segments_to_visulize, i+1)\n",
    "        plt.bar(np.arange(_encoding.shape[1]), _encoding[i,:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
