{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import train\n",
    "import data_processor as dp\n",
    "import utils\n",
    "import models\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import importlib\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADER\n",
    "\n",
    "# Block Vars\n",
    "_quiet = True\n",
    "\n",
    "# Load Model\n",
    "model_name = 'conv_ae_shared'\n",
    "classifier_block = 'features'\n",
    "classifier_layer_index = 30\n",
    "train_config_path = {\n",
    "  'conv_ae_shared_test': 'models/test/conv_autoencoder_shared/config.json',\n",
    "  'classifier_test': 'models/test/classifier_vgg16/config.json',\n",
    "  'conv_ae_shared' : 'models/hp_tune_ae/conv_ae_mix/shared/config.json',\n",
    "  'conv_ae_skip' : 'models/hp_tune_ae/conv_ae_mix/skip/config.json',\n",
    "  'conv_ae_not_shared' : 'models/hp_tune_ae/conv_ae_mix/not_shared/config.json',\n",
    "  'alexnet' : 'models/hp_tune_classifier/classifier_mix/alexnet_2/config.json',\n",
    "  'vgg11' : 'models/hp_tune_classifier/classifier_mix/vgg11_2/config.json',\n",
    "  'vgg13' : 'models/hp_tune_classifier/classifier_mix/vgg13_2/config.json',\n",
    "  'vgg16' : 'models/hp_tune_classifier/classifier_mix/vgg16_2/config.json'\n",
    "}[model_name]\n",
    "train_config = train.TrainingConfig.load_from_file(train_config_path)\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "model = train_config.get_by_model_key(cuda)\n",
    "checkpoint = models.ModelCheckpoint(model)\n",
    "model.load_state(train_config.get_model_path('state_best'))\n",
    "checkpoint.load(train_config.get_model_path('checkpoint_best'))\n",
    "if not _quiet:\n",
    "    print('Model [{}] loaded with weights. Cuda:{}.\\nConfig:\\n{}\\nCheckpoint:\\n{}\\n'\n",
    "          .format(train_config.name, cuda, train_config.get_dict(), checkpoint.get_dict()))\n",
    "    \n",
    "# Analysis Dir\n",
    "analysis_dir = os.path.join(train_config.models_dir, 'analysis')\n",
    "os.makedirs(analysis_dir, exist_ok=True)\n",
    "if not _quiet:\n",
    "    print(\"Analysis dir: {}\".format(analysis_dir))\n",
    "\n",
    "# Load Dataset\n",
    "dataset_config = dp.DataPrepConfig.load_from_dataset(train_config.dataset_path)\n",
    "train_parts, cv_part, test_part = dp.load_created_partitions(train_config.dataset_path)\n",
    "if test_part.get_num_segments() == 0:\n",
    "    raise Exception('No data in test set')\n",
    "if not _quiet:\n",
    "    print('Dataset [{}] loaded. Config:\\n{}\\n'.format(dataset_config.name, dataset_config.get_dict()))\n",
    "\n",
    "test_set = dp.PartitionBatchGenerator(test_part, train_config.batch_size, mode='test')\n",
    "test_set_len = len(test_set)\n",
    "if not _quiet:\n",
    "    print('Test Set Loaded. Batch Size:{} Num Batches:{}'.format(test_set.batch_size, test_set_len))\n",
    "\n",
    "# Load Tracks\n",
    "tracks = utils.get_fma_meta(dataset_config.fma_meta_dir, dataset_config.fma_type)\n",
    "if not _quiet:\n",
    "    print('FMA metadata loaded. Shape {}'.format(tracks.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "def eval_test():\n",
    "    loss_test = 0\n",
    "    model.begin_evaluation()\n",
    "    for x_test, y_test in test_set:\n",
    "        loss_batch_test = model.evaluate(x_test, y_test)\n",
    "        loss_test += loss_batch_test\n",
    "    avg_loss_test = loss_test / test_set_len\n",
    "    print('Average test loss per batch:', avg_loss_test)\n",
    "    model.post_evaluation()\n",
    "if False:\n",
    "    eval_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE ENCODINGS\n",
    "def get_test_enc(train_config, test_set, classifier_block=None, classifier_layer_index=None, quiet=False):\n",
    "    test_enc = None\n",
    "    enc_segs = None\n",
    "    for x_test, y_test in test_set:\n",
    "        with torch.no_grad():\n",
    "            if train_config.model == 'cnn_classifier':\n",
    "                enc = model.encode(x_test, classifier_block, classifier_layer_index)\n",
    "            elif train_config.model == 'conv_autoencoder':\n",
    "                enc = model.encode(x_test)\n",
    "            test_enc = enc.cpu().numpy() if test_enc is None else np.concatenate([test_enc, enc.cpu().numpy()])\n",
    "            enc_segs = y_test if enc_segs is None else np.concatenate([enc_segs, y_test])\n",
    "    if not quiet: print('Test set encoding shape: {}'.format(test_enc.shape))\n",
    "    test_enc = test_enc.reshape(test_enc.shape[0], -1)\n",
    "    if not quiet: print('Test set encoding reshaped: {}'.format(test_enc.shape))\n",
    "    return test_enc, enc_segs\n",
    "    \n",
    "if True:\n",
    "    _load_cached = True\n",
    "    _cache_dir = os.path.join(analysis_dir, 'cached')\n",
    "    os.makedirs(_cache_dir, exist_ok=True)\n",
    "    _enc_file = os.path.join(_cache_dir, 'test_enc.npy')\n",
    "    _segs_file = os.path.join(_cache_dir, 'enc_segs.npy')\n",
    "    if _load_cached and os.path.isfile(_enc_file) and os.path.isfile(_segs_file):\n",
    "        print('Loading saved encodings')\n",
    "        test_enc, enc_segs = np.load(_enc_file), np.load(_segs_file)\n",
    "    else:\n",
    "        print('Generating encodings')\n",
    "        test_enc, enc_segs = get_test_enc(train_config, test_set, classifier_block, classifier_layer_index)\n",
    "        np.save(_enc_file, test_enc), np.save(_segs_file, enc_segs)\n",
    "    print(test_enc.shape, enc_segs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE ENCODINGS\n",
    "def get_enc_scaled(enc, mode='all'):\n",
    "    assert mode in ['features', 'across']\n",
    "    enc_min = enc.min(axis=0) if mode == 'features' else enc.reshape(-1).min()\n",
    "    enc_max = enc.max(axis=0) if mode == 'features' else enc.reshape(-1).max()\n",
    "    return (enc - enc_min) / (enc_max - enc_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING STATS\n",
    "\n",
    "# Overall stats\n",
    "def print_enc_stats(enc, max_segs=1000000, max_encs=200, save_plots=False, save_dir=None, save_file_prefix=None):\n",
    "    print('Num segments:', enc.shape[0])\n",
    "    print('Distribution across entire encoding')\n",
    "    print(pd.Series(enc.reshape(-1)).describe())\n",
    "    \n",
    "    pylab.rcParams['figure.figsize'] = (14,8)\n",
    "    \n",
    "    enc_idx = np.arange(enc.shape[1])\n",
    "    if enc_idx.size > max_encs:\n",
    "        print('Keeping only {} components'.format(max_encs))\n",
    "        np.random.shuffle(enc_idx)\n",
    "        enc_idx = enc_idx[:max_encs]\n",
    "        enc_idx.sort()\n",
    "        enc = enc[:, enc_idx]\n",
    "    if enc.shape[0] > max_segs:\n",
    "        print('Keeping only {} segments'.format(max_segs))\n",
    "        idx = np.arange(enc.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:max_segs]\n",
    "        enc = enc[:max_segs, :]\n",
    "\n",
    "    print('Plotting stats for {} components'.format(enc_idx.size))\n",
    "\n",
    "    x_label = 'Encoding Component'\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Mean')\n",
    "    plt.bar(enc_idx, enc.mean(axis=0))\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Min')\n",
    "    plt.bar(enc_idx, enc.min(axis=0))\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Max')\n",
    "    plt.bar(enc_idx, enc.max(axis=0))\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Variance')\n",
    "    plt.bar(enc_idx, enc.var(axis=0))\n",
    "\n",
    "    if save_plots:\n",
    "        path = os.path.join(analysis_dir, save_dir, \"{}.desc.jpg\".format(save_file_prefix))\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        plt.savefig(path, dpi=300)\n",
    "        print('Plots saved to: {}'.format(path))\n",
    "    plt.show()\n",
    "    \n",
    "    print('Plotting percentiles {} components'.format(enc_idx.size))\n",
    "    pylab.rcParams['figure.figsize'] = (14,12)\n",
    "    percentiles = [10, 30, 50 ,70, 90, 100]\n",
    "    for i, p in enumerate(percentiles):\n",
    "        plt.subplot(3, 2, i+1)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel('{} Percentile'.format(p))\n",
    "        plt.bar(enc_idx, np.percentile(enc, p, axis=0))\n",
    "    if save_plots:\n",
    "        path = os.path.join(analysis_dir, save_dir, \"{}.percetiles.jpg\".format(save_file_prefix))\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        plt.savefig(path, dpi=300)\n",
    "        print('Plots saved to: {}'.format(path))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW ENCODING ANALYSIS\n",
    "if True:\n",
    "    _stats_dir = 'raw'\n",
    "    for _scale in [None, 'features', 'across']:\n",
    "        print('\\n\\nRaw Encoding Analysis. Scale: {}\\n\\n'.format(_scale))\n",
    "        if _scale is None:\n",
    "            _file_prefix = 'unscaled.stats'\n",
    "            enc_scaled = test_enc\n",
    "        else:\n",
    "            enc_scaled = get_enc_scaled(test_enc, _scale)\n",
    "            _file_prefix = 'scaled_{}.stats'.format(_scale)\n",
    "        print_enc_stats(enc_scaled, save_plots=True, save_dir=_stats_dir, save_file_prefix=_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING SCATTER PLOTS\n",
    "\n",
    "def show_enc_scatter(enc, num_plots=10):\n",
    "    pylab.rcParams['figure.figsize'] = (20, 20)\n",
    "    dims_x = np.random.randint(0, enc.shape[1], num_plots)\n",
    "    dims_y = np.random.randint(0, enc.shape[1], num_plots)\n",
    "    for i in range(num_plots):\n",
    "        dim1, dim2 = dims_x[i], dims_y[i]\n",
    "        x = np.transpose(enc[:, dim1])\n",
    "        y = np.transpose(enc[:, dim2])\n",
    "        plt.subplot(int(num_plots/3)+1, 3, i+1)\n",
    "        plt.xlabel('Dim {0}'.format(dim1))\n",
    "        plt.ylabel('Dim {0}'.format(dim2))\n",
    "        plt.scatter(x, y, marker='^', c='blue')\n",
    "if False:\n",
    "    show_enc_scatter(enc_scaled, num_plots=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "def get_enc_pca(enc, reduced_dims, save=False, load=False, save_dir=None, save_file_prefix=None):\n",
    "\n",
    "    model_path = os.path.join(analysis_dir, save_dir, '{}.pca.model'.format(save_file_prefix))\n",
    "    if load and os.path.isfile(model_path):\n",
    "        with open(model_path, 'rb') as modfile:\n",
    "            print('Loading saved model {}'.format(model_path))\n",
    "            pca = pickle.load(modfile)\n",
    "    else:\n",
    "        pca = sklearn.decomposition.PCA(n_components=reduced_dims)\n",
    "        pca.fit(enc)\n",
    "        if save:\n",
    "            os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "            with open(model_path, 'wb') as modfile:\n",
    "                pickle.dump(pca, modfile)\n",
    "                print('Model saved to: {}'.format(model_path))\n",
    "            \n",
    "    enc_pca = pca.transform(enc)\n",
    "    print('Variance retained: {}%'.format(pca.explained_variance_ratio_.sum()*100))\n",
    "    if True:\n",
    "        print('Variance by components')\n",
    "        print(pca.explained_variance_ratio_.cumsum())\n",
    "    return enc_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA ANALYSIS\n",
    "if True:\n",
    "    _pca_model_prefix = train_config.name\n",
    "    _save_dir = 'pca'\n",
    "    enc_pca = get_enc_pca(test_enc, 10, save=True, load=True, save_dir=_save_dir, save_file_prefix=_pca_model_prefix)\n",
    "    for _scale in [None, 'features', 'across']:\n",
    "        print('\\n\\nPCA Analysis. Scale: {}\\n\\n'.format(_scale))\n",
    "        if _scale is None:\n",
    "            _stats_file_prefix = 'pca.unscaled.stats'\n",
    "        else:\n",
    "            enc_pca = get_enc_scaled(enc_pca, _scale)\n",
    "            _stats_file_prefix = 'pca.scaled_{}.stats'.format(_scale)\n",
    "        print_enc_stats(enc_pca, save_plots=True, save_dir=_save_dir, save_file_prefix=_stats_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST CLUSTER\n",
    "\n",
    "def get_best_cluster(enc, try_clusters=15):\n",
    "\n",
    "    cluster_range = range( 1, try_clusters )\n",
    "    cluster_errors = []\n",
    "\n",
    "    for num_clusters in cluster_range:\n",
    "        print('Checking cluster {} of {}'.format(num_clusters+1, try_clusters))\n",
    "        clusters = sklearn.cluster.KMeans(num_clusters)\n",
    "        clusters.fit(enc)\n",
    "        cluster_errors.append(clusters.inertia_)\n",
    "\n",
    "    clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\n",
    "    print('Cluster Errors')\n",
    "    print(clusters_df)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )\n",
    "if False:\n",
    "    get_best_cluster(enc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS\n",
    "\n",
    "def get_enc_kmeans(enc, reduced_dims, softmax=True, save=False, load=False, save_dir=None, save_file_prefix=None):\n",
    "    \n",
    "    model_path = os.path.join(analysis_dir, save_dir, '{}.kmeans.model'.format(save_file_prefix))\n",
    "    if load and os.path.isfile(model_path):\n",
    "        with open(model_path, 'rb') as modfile:\n",
    "            print('Loading saved model {}'.format(model_path))\n",
    "            kmeans = pickle.load(modfile)\n",
    "    else:\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=reduced_dims)\n",
    "        kmeans.fit(enc)\n",
    "        if save:\n",
    "            os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "            with open(model_path, 'wb') as modfile:\n",
    "                pickle.dump(kmeans, modfile)\n",
    "                print('Model saved to: {}'.format(model_path))\n",
    "    \n",
    "    enc_kmeans = kmeans.transform(enc)\n",
    "    print('Score', kmeans.score(enc))\n",
    "    print('Data transformed', pd.Series(enc_kmeans.reshape(-1)).describe())\n",
    "    enc_kmeans = 1 / (1 + enc_kmeans)\n",
    "    print('Data similarity', pd.Series(enc_kmeans.reshape(-1)).describe())\n",
    "    if softmax:\n",
    "        enc_kmeans = np.exp(enc_kmeans) / np.exp(enc_kmeans).sum(axis=1, keepdims=True)\n",
    "        print('Softmax similarity', pd.Series(enc_kmeans.reshape(-1)).describe())\n",
    "    return enc_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS ANALYSIS\n",
    "if True:\n",
    "    _kmeans_model_prefix = train_config.name\n",
    "    _save_dir = 'kmeans'\n",
    "    for _kmeans_softmax in [True, False]:\n",
    "        enc_kmeans = get_enc_kmeans(test_enc, 10, softmax=_kmeans_softmax, save=True, load=True, save_dir=_save_dir, save_file_prefix=_kmeans_model_prefix)\n",
    "        for _scale in [None, 'features', 'across']:\n",
    "            print('\\n\\nK-Means Analysis. Scale: {}\\n\\n'.format(_scale))\n",
    "            if _scale is None:\n",
    "                _stats_file_prefix = 'kmeans{}.unscaled.stats'.format('.softmax' if _kmeans_softmax else '')\n",
    "            else:\n",
    "                enc_kmeans = get_enc_scaled(enc_kmeans, _scale)\n",
    "                _stats_file_prefix = 'kmeans{}.scaled_{}.stats'.format('.softmax' if _kmeans_softmax else '', _scale)\n",
    "            print_enc_stats(enc_kmeans, save_plots=True, save_dir=_save_dir, save_file_prefix=_stats_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS PCA ANALYSIS\n",
    "if True:\n",
    "    _kmeans_model_prefix = train_config.name\n",
    "    _save_dir_kmeans = 'kmeans'\n",
    "    _save_dir_kmeans_pca = 'kmeans-pca'\n",
    "    for _kmeans_softmax in [True, False]:\n",
    "        _pca_model_prefix = \"{}.kmeans{}-pca\".format(train_config.name, '-softmax' if _kmeans_softmax else '')\n",
    "        enc_kmeans = get_enc_kmeans(test_enc, 10, softmax=_kmeans_softmax, save=True, load=True, save_dir=_save_dir_kmeans, save_file_prefix=_kmeans_model_prefix)\n",
    "        enc_pca = get_enc_pca(enc_kmeans, 10, save=True, load=True, save_dir=_save_dir_kmeans_pca, save_file_prefix=_pca_model_prefix)\n",
    "        for _scale in [None, 'features', 'across']:\n",
    "            print('\\n\\nK-Means PCA Analysis. Scale: {}\\n\\n'.format(_scale))\n",
    "            if _scale is None:\n",
    "                _stats_file_prefix = 'kmeans-pca{}.unscaled.stats'.format('.softmax' if _kmeans_softmax else '')\n",
    "            else:\n",
    "                enc_pca = get_enc_scaled(enc_kmeans, _scale)\n",
    "                _stats_file_prefix = 'kmeans-pca{}.scaled_{}.stats'.format('.softmax' if _kmeans_softmax else '', _scale)\n",
    "            print_enc_stats(enc_pca, save_plots=True, save_dir=_save_dir_kmeans_pca, save_file_prefix=_stats_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done\n",
    "import IPython\n",
    "IPython.display.Audio(\"/home/rb/hdd/rsrcs/sounds/sms.mp3\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepviz",
   "language": "python",
   "name": "deepviz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
