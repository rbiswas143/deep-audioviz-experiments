{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import train\n",
    "import data_processor as dp\n",
    "import commons\n",
    "import models\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import importlib\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADER\n",
    "\n",
    "# Block Vars\n",
    "_quiet = False\n",
    "\n",
    "# Pick Model\n",
    "all_models = [\n",
    "    ('conv_ae_shared_test', None, True),  #0\n",
    "    ('classifier_test', 'L15', True),  #1\n",
    "    ('conv_ae_shared', None, True),  #2\n",
    "    ('conv_ae_skip', None, True),  #3\n",
    "    ('conv_ae_not_shared', None, True),  #4\n",
    "    ('alexnet', 'L6', True),  #5\n",
    "    ('vgg11', 'L8', True),  #6\n",
    "    ('vgg13', 'L10', True),  #7\n",
    "    ('vgg16', 'L13', True),  #8\n",
    "    ('vgg16', 'L14', True),  #9\n",
    "    ('vgg16', 'L15', True),  #10\n",
    "]\n",
    "\n",
    "curr_model = 0\n",
    "fresh_analysis = False # Warning: this deletes all saved data for the current model including encodings, clusters, etc\n",
    "compute_stats = False\n",
    "\n",
    "model_name, model_layer, do_kmeans = all_models[curr_model]\n",
    "print('Model: {}\\tLayer: {}\\tDo K-Means: {}'.format(*all_models[curr_model]))\n",
    "classifier_block, classifier_layer_index = None, None\n",
    "try:\n",
    "    classifier_block, classifier_layer_index = models.encoding_layer_options[model_name][model_layer]\n",
    "except:\n",
    "    print('No layer info for current model')\n",
    "\n",
    "# Load config\n",
    "train_config_path = models.trained_model_configs[model_name]\n",
    "train_config = train.TrainingConfig.load_from_file(train_config_path)\n",
    "\n",
    "# Load model\n",
    "cuda = torch.cuda.is_available()\n",
    "# cuda = False\n",
    "model = train_config.get_by_model_key(cuda)\n",
    "checkpoint = models.ModelCheckpoint(model)\n",
    "model.load_state(train_config.get_model_path('state_best'))\n",
    "checkpoint.load(train_config.get_model_path('checkpoint_best'))\n",
    "if not _quiet:\n",
    "    print('Model [{}] loaded with weights. Cuda:{}.\\nConfig:\\n{}\\nCheckpoint:\\n{}\\n'\n",
    "          .format(train_config.name, cuda, train_config.get_dict(), checkpoint.get_dict()))\n",
    "    \n",
    "# Analysis Dir\n",
    "analysis_dir = os.path.join(train_config.models_dir, 'analysis')\n",
    "if model_layer is not None:\n",
    "    analysis_dir = os.path.join(analysis_dir, model_layer)\n",
    "if fresh_analysis:\n",
    "    shutil.rmtree(analysis_dir, ignore_errors=True)\n",
    "    if not _quiet:\n",
    "        print('Analysis directory has been deleted if it existed')\n",
    "os.makedirs(analysis_dir, exist_ok=True)\n",
    "if not _quiet:\n",
    "    print(\"Analysis dir: {}\".format(analysis_dir))\n",
    "\n",
    "# Load Dataset\n",
    "dataset_config = dp.DataPrepConfig.load_from_dataset(train_config.dataset_path)\n",
    "train_parts, cv_part, test_part = dp.load_created_partitions(train_config.dataset_path)\n",
    "if test_part.get_num_segments() == 0:\n",
    "    raise Exception('No data in test set')\n",
    "if not _quiet:\n",
    "    print('Dataset [{}] loaded. Config:\\n{}\\n'.format(dataset_config.name, dataset_config.get_dict()))\n",
    "\n",
    "test_set = dp.PartitionBatchGenerator(test_part, train_config.batch_size, mode='test')\n",
    "test_set_len = len(test_set)\n",
    "if not _quiet:\n",
    "    print('Test Set Loaded. Batch Size:{} Num Batches:{}'.format(test_set.batch_size, test_set_len))\n",
    "\n",
    "# Load Tracks\n",
    "tracks = commons.get_fma_meta(dataset_config.fma_meta_dir, dataset_config.fma_type)\n",
    "if not _quiet:\n",
    "    print('FMA metadata loaded. Shape {}'.format(tracks.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "def eval_test():\n",
    "    loss_test = 0\n",
    "    model.begin_evaluation()\n",
    "    for x_test, y_test in test_set:\n",
    "        loss_batch_test = model.evaluate(x_test, y_test)\n",
    "        loss_test += loss_batch_test\n",
    "    avg_loss_test = loss_test / test_set_len\n",
    "    print('Average test loss per batch:', avg_loss_test)\n",
    "    model.post_evaluation()\n",
    "if False:\n",
    "    eval_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE ENCODINGS\n",
    "def get_test_enc(train_config, test_set, classifier_block=None, classifier_layer_index=None, quiet=False):\n",
    "    test_enc = None\n",
    "    enc_segs = None\n",
    "    for x_test, y_test in test_set:\n",
    "        with torch.no_grad():\n",
    "            if train_config.model == 'cnn_classifier':\n",
    "                enc = model.encode(x_test, classifier_block, classifier_layer_index)\n",
    "            elif train_config.model == 'conv_autoencoder':\n",
    "                enc = model.encode(x_test)\n",
    "            test_enc = enc.cpu().numpy() if test_enc is None else np.concatenate([test_enc, enc.cpu().numpy()])\n",
    "            enc_segs = y_test if enc_segs is None else np.concatenate([enc_segs, y_test])\n",
    "    if not quiet: print('Test set encoding shape: {}'.format(test_enc.shape))\n",
    "    test_enc = test_enc.reshape(test_enc.shape[0], -1)\n",
    "    if not quiet: print('Test set encoding reshaped: {}'.format(test_enc.shape))\n",
    "    return test_enc, enc_segs\n",
    "    \n",
    "def get_or_load_test_enc():\n",
    "    _load_cached = True\n",
    "    _cache_dir = os.path.join(analysis_dir, 'cached')\n",
    "    os.makedirs(_cache_dir, exist_ok=True)\n",
    "    _enc_file = os.path.join(_cache_dir, 'test_enc.npy')\n",
    "    _segs_file = os.path.join(_cache_dir, 'enc_segs.npy')\n",
    "    if _load_cached and os.path.isfile(_enc_file) and os.path.isfile(_segs_file):\n",
    "        print('Loading saved encodings')\n",
    "        test_enc, enc_segs = np.load(_enc_file), np.load(_segs_file)\n",
    "    else:\n",
    "        print('Generating encodings')\n",
    "        test_enc, enc_segs = get_test_enc(train_config, test_set, classifier_block, classifier_layer_index)\n",
    "        np.save(_enc_file, test_enc), np.save(_segs_file, enc_segs)\n",
    "    print(test_enc.shape, enc_segs.shape)\n",
    "    return test_enc, enc_segs\n",
    "\n",
    "if True:\n",
    "    test_enc, enc_segs = get_or_load_test_enc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE ENCODINGS\n",
    "def get_enc_scaled(enc, mode='across', method='standard', std_scale=0.4, save=False, load=False, save_dir=None, prefix=None):\n",
    "    assert mode in ['features', 'across']\n",
    "    assert method in ['minmax', 'standard']\n",
    "    print('Scaling encoding with mode: {} and method: {}'.format(mode, method))\n",
    "    \n",
    "    enc_shape = enc.shape\n",
    "    if mode == 'across':\n",
    "        enc = enc.reshape(-1, 1)\n",
    "    \n",
    "    if save_dir is not None:\n",
    "        scaler_path = os.path.join(save_dir, '{}.{}.{}.scaler'.format(prefix, mode, method))\n",
    "    if load and os.path.isfile(scaler_path):\n",
    "        with open(scaler_path, 'rb') as modfile:\n",
    "            print('Loading saved scaler {}'.format(scaler_path))\n",
    "            scaler = pickle.load(modfile)\n",
    "    else:\n",
    "        if method == 'standard':\n",
    "            scaler = sklearn.preprocessing.StandardScaler()\n",
    "        else:\n",
    "            scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "        scaler.fit(enc)\n",
    "        if save:\n",
    "            os.makedirs(os.path.dirname(scaler_path), exist_ok=True)\n",
    "            with open(scaler_path, 'wb') as modfile:\n",
    "                pickle.dump(scaler, modfile)\n",
    "                print('Scaler saved to: {}'.format(scaler_path))\n",
    "    enc = scaler.transform(enc)\n",
    "    if method == 'standard':\n",
    "        enc = (enc * std_scale) + 0.5  # Scale between 0 and 1\n",
    "        enc = np.clip(enc, 0, 1)\n",
    "    \n",
    "    return enc.reshape(enc_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING STATS\n",
    "\n",
    "# Overall stats\n",
    "def print_enc_stats(enc, max_segs=1000000, max_encs=10000, dpi=80, save_plots=False, save_dir=None, save_file_prefix=None):\n",
    "    print('Num segments:', enc.shape[0])\n",
    "    print('Distribution across entire encoding')\n",
    "    print(pd.Series(enc.reshape(-1)).describe())\n",
    "    \n",
    "    pylab.rcParams['figure.figsize'] = (14,8)\n",
    "    \n",
    "    enc_idx = np.arange(enc.shape[1])\n",
    "    if enc_idx.size > max_encs:\n",
    "        print('Keeping only {} components'.format(max_encs))\n",
    "        np.random.shuffle(enc_idx)\n",
    "        enc_idx = enc_idx[:max_encs]\n",
    "        enc_idx.sort()\n",
    "        enc = enc[:, enc_idx]\n",
    "    if enc.shape[0] > max_segs:\n",
    "        print('Keeping only {} segments'.format(max_segs))\n",
    "        idx = np.arange(enc.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:max_segs]\n",
    "        enc = enc[:max_segs, :]\n",
    "\n",
    "    print('Plotting stats for {} components'.format(enc_idx.size))\n",
    "\n",
    "    x_label = 'Encoding Component'\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Mean')\n",
    "    plt.bar(enc_idx, enc.mean(axis=0))\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Min')\n",
    "    plt.bar(enc_idx, enc.min(axis=0))\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Max')\n",
    "    plt.bar(enc_idx, enc.max(axis=0))\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Standard Deviation')\n",
    "    plt.bar(enc_idx, enc.std(axis=0))\n",
    "\n",
    "    if save_plots:\n",
    "        path = os.path.join(save_dir, \"{}.desc.jpg\".format(save_file_prefix))\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        plt.savefig(path, dpi=dpi)\n",
    "        print('Plots saved to: {}'.format(path))\n",
    "    plt.show()\n",
    "    \n",
    "#     print('Plotting percentiles {} components'.format(enc_idx.size))\n",
    "#     pylab.rcParams['figure.figsize'] = (14,12)\n",
    "#     percentiles = [10, 30, 50 ,70, 90, 100]\n",
    "#     for i, p in enumerate(percentiles):\n",
    "#         plt.subplot(3, 2, i+1)\n",
    "#         plt.xlabel(x_label)\n",
    "#         plt.ylabel('{} Percentile'.format(p))\n",
    "#         plt.bar(enc_idx, np.percentile(enc, p, axis=0))\n",
    "#     if save_plots:\n",
    "#         path = os.path.join(save_dir, \"{}.percetiles.jpg\".format(save_file_prefix))\n",
    "#         os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#         plt.savefig(path, dpi=dpi)\n",
    "#         print('Plots saved to: {}'.format(path))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW ENCODING ANALYSIS\n",
    "def raw_enc_analysis(test_enc, save_plots=True, save_dir=None, prefix='', scaler_dir=None, compute_stats=True):\n",
    "    if scaler_dir is None:\n",
    "        scaler_dir = save_dir\n",
    "    for _scale, _method in ([(None, None)] + list(itertools.product(['features', 'across'], ['minmax', 'standard']))):\n",
    "        print('\\n\\nRaw Encoding Analysis. Scale: {}. Method: {}\\n\\n'.format(_scale, _method))\n",
    "        if _scale is None:\n",
    "            _file_prefix = '{}unscaled.stats'.format(prefix)\n",
    "            enc_scaled = test_enc\n",
    "        else:\n",
    "            enc_scaled = get_enc_scaled(test_enc, _scale, _method, save=True, load=True, save_dir=scaler_dir, prefix='stats')\n",
    "            _file_prefix = '{}scaled_{}.method_{}.stats'.format(prefix, _scale, _method)\n",
    "        if compute_stats:\n",
    "            print_enc_stats(enc_scaled, save_plots=save_plots, save_dir=save_dir, save_file_prefix=_file_prefix)\n",
    "if True:\n",
    "    raw_enc_analysis(test_enc, save_dir=os.path.join(analysis_dir, 'raw'), compute_stats=compute_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING SCATTER PLOTS\n",
    "\n",
    "def show_enc_scatter(enc, num_plots=10):\n",
    "    pylab.rcParams['figure.figsize'] = (20, 20)\n",
    "    dims_x = np.random.randint(0, enc.shape[1], num_plots)\n",
    "    dims_y = np.random.randint(0, enc.shape[1], num_plots)\n",
    "    for i in range(num_plots):\n",
    "        dim1, dim2 = dims_x[i], dims_y[i]\n",
    "        x = np.transpose(enc[:, dim1])\n",
    "        y = np.transpose(enc[:, dim2])\n",
    "        plt.subplot(int(num_plots/3)+1, 3, i+1)\n",
    "        plt.xlabel('Dim {0}'.format(dim1))\n",
    "        plt.ylabel('Dim {0}'.format(dim2))\n",
    "        plt.scatter(x, y, marker='^', c='blue')\n",
    "if False:\n",
    "    show_enc_scatter(enc_scaled, num_plots=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "def get_enc_pca(enc, reduced_dims, save=False, load=False, save_dir=None, save_file_prefix=None):\n",
    "\n",
    "    scaler_path = os.path.join(save_dir, '{}.pca.scaler'.format(save_file_prefix))\n",
    "    pca_model_path = os.path.join(save_dir, '{}.pca.model'.format(save_file_prefix))\n",
    "    if load and os.path.isfile(pca_model_path) and os.path.isfile(scaler_path):\n",
    "        with open(scaler_path, 'rb') as modfile:\n",
    "            print('Loading saved scaler {}'.format(scaler_path))\n",
    "            scaler = pickle.load(modfile)\n",
    "            enc_scaled = scaler.transform(enc)\n",
    "        with open(pca_model_path, 'rb') as modfile:\n",
    "            print('Loading saved model {}'.format(pca_model_path))\n",
    "            pca = pickle.load(modfile)\n",
    "    else:\n",
    "        scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "        enc_scaled = scaler.fit_transform(enc)\n",
    "        pca = sklearn.decomposition.PCA(n_components=reduced_dims)\n",
    "        pca.fit(enc_scaled)\n",
    "        if save:\n",
    "            os.makedirs(os.path.dirname(scaler_path), exist_ok=True)\n",
    "            with open(scaler_path, 'wb') as modfile:\n",
    "                pickle.dump(scaler, modfile)\n",
    "                print('Scaler saved to: {}'.format(scaler_path))\n",
    "            with open(pca_model_path, 'wb') as modfile:\n",
    "                pickle.dump(pca, modfile)\n",
    "                print('Model saved to: {}'.format(pca_model_path))\n",
    "            \n",
    "    enc_pca = pca.transform(enc_scaled)\n",
    "    print('Variance retained: {}%'.format(pca.explained_variance_ratio_.sum()*100))\n",
    "    if True:\n",
    "        print('Variance by components')\n",
    "        print(pca.explained_variance_ratio_.cumsum())\n",
    "    return enc_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA ANALYSIS\n",
    "def pca_analysis(test_enc, model_dir=None, save_plots=True, save_dir=None, prefix='', scaler_dir=None, compute_stats=True):\n",
    "    if save_dir is None:\n",
    "        save_dir = model_dir\n",
    "    if scaler_dir is None:\n",
    "        scaler_dir = model_dir\n",
    "    _pca_model_prefix = train_config.name\n",
    "    enc_pca = get_enc_pca(test_enc, 10, save=True, load=True, save_dir=model_dir, save_file_prefix=_pca_model_prefix)\n",
    "    for _scale, _method in ([(None, None)] + list(itertools.product(['features', 'across'], ['minmax', 'standard']))):\n",
    "        print('\\n\\nPCA Analysis. Scale: {}. Method: {}\\n\\n'.format(_scale, _method))\n",
    "        if _scale is None:\n",
    "            _stats_file_prefix = '{}pca.unscaled.stats'.format(prefix)\n",
    "            enc_scaled = enc_pca\n",
    "        else:\n",
    "            enc_scaled = get_enc_scaled(enc_pca, _scale, _method, save=True, load=True, save_dir=scaler_dir, prefix=_pca_model_prefix)\n",
    "            _stats_file_prefix = '{}pca.scaled_{}.method_{}.stats'.format(prefix, _scale, _method)\n",
    "        if compute_stats:\n",
    "            print_enc_stats(enc_scaled, save_plots=save_plots, save_dir=save_dir, save_file_prefix=_stats_file_prefix)\n",
    "if True:\n",
    "    pca_analysis(test_enc, model_dir=os.path.join(analysis_dir, 'pca'), compute_stats=compute_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BEST CLUSTER\n",
    "\n",
    "def get_best_cluster(enc, try_clusters=10):\n",
    "    \n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    enc_scaled = scaler.fit_transform(enc)\n",
    "\n",
    "    cluster_range = range( 1, try_clusters )\n",
    "    cluster_errors = []\n",
    "\n",
    "    for num_clusters in cluster_range:\n",
    "        print('Checking cluster {} of {}'.format(num_clusters+1, try_clusters))\n",
    "        clusters = sklearn.cluster.KMeans(num_clusters)\n",
    "        clusters.fit(enc_scaled)\n",
    "        cluster_errors.append(clusters.inertia_)\n",
    "\n",
    "    clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\n",
    "    print('Cluster Errors')\n",
    "    print(clusters_df)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )\n",
    "if False:\n",
    "    get_best_cluster(test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS\n",
    "\n",
    "def get_enc_kmeans(enc, reduced_dims, save=False, load=False, save_dir=None, save_file_prefix=None):\n",
    "            \n",
    "    scaler_path = os.path.join(save_dir, '{}.kmeans.scaler'.format(save_file_prefix))\n",
    "    model_path = os.path.join(save_dir, '{}.kmeans.model'.format(save_file_prefix))\n",
    "    if load and os.path.isfile(model_path) and os.path.isfile(scaler_path):\n",
    "        with open(scaler_path, 'rb') as modfile:\n",
    "            print('Loading saved scaler {}'.format(scaler_path))\n",
    "            scaler = pickle.load(modfile)\n",
    "            enc_scaled = scaler.transform(enc)\n",
    "        with open(model_path, 'rb') as modfile:\n",
    "            print('Loading saved model {}'.format(model_path))\n",
    "            kmeans = pickle.load(modfile)\n",
    "    else:\n",
    "        scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "        enc_scaled = scaler.fit_transform(enc)\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=reduced_dims)\n",
    "        kmeans.fit(enc_scaled)\n",
    "        if save:\n",
    "            os.makedirs(os.path.dirname(scaler_path), exist_ok=True)\n",
    "            with open(scaler_path, 'wb') as modfile:\n",
    "                pickle.dump(scaler, modfile)\n",
    "                print('Scaler saved to: {}'.format(scaler_path))\n",
    "            with open(model_path, 'wb') as modfile:\n",
    "                pickle.dump(kmeans, modfile)\n",
    "                print('Model saved to: {}'.format(model_path))\n",
    "                \n",
    "    enc_kmeans = kmeans.transform(enc_scaled)\n",
    "    print('Score', kmeans.score(enc))\n",
    "    print('Data transformed', pd.Series(enc_kmeans.reshape(-1)).describe())\n",
    "    enc_kmeans = 1 / (enc_kmeans)\n",
    "    print('Data similarity', pd.Series(enc_kmeans.reshape(-1)).describe())\n",
    "    \n",
    "    return enc_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS ANALYSIS\n",
    "def kmeans_analysis(test_enc, model_dir=None, save_plots=True, save_dir=None, prefix='', scaler_dir=None, compute_stats=True):\n",
    "    if not do_kmeans:\n",
    "        print('K-Means analysis is not allowed')\n",
    "        return\n",
    "    \n",
    "    if save_dir is None:\n",
    "        save_dir = model_dir\n",
    "    if scaler_dir is None:\n",
    "        scaler_dir = model_dir\n",
    "    _kmeans_model_prefix = train_config.name\n",
    "    enc_kmeans = get_enc_kmeans(test_enc, 10, save=True, load=True, save_dir=model_dir, save_file_prefix=_kmeans_model_prefix)\n",
    "    for _scale, _method in ([(None, None)] + list(itertools.product(['features', 'across'], ['minmax', 'standard']))):\n",
    "        print('\\n\\nK-Means Analysis. Scale: {}. Method: {}\\n\\n'.format(_scale, _method))\n",
    "        if _scale is None:\n",
    "            _stats_file_prefix = '{}kmeans.unscaled.stats'.format(prefix)\n",
    "            enc_scaled = enc_kmeans\n",
    "        else:\n",
    "            scaler_prefix = _kmeans_model_prefix\n",
    "            enc_scaled = get_enc_scaled(enc_kmeans, _scale, _method, save=True, load=True, save_dir=scaler_dir, prefix=scaler_prefix)\n",
    "            _stats_file_prefix = '{}kmeans.scaled_{}.method_{}.stats'.format(prefix, _scale, _method)\n",
    "        if compute_stats:\n",
    "            print_enc_stats(enc_scaled, save_plots=save_plots, save_dir=save_dir, save_file_prefix=_stats_file_prefix)\n",
    "if True:\n",
    "    kmeans_analysis(test_enc, model_dir=os.path.join(analysis_dir, 'kmeans'), compute_stats=compute_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS PCA ANALYSIS\n",
    "def kmeans_pca_analysis(test_enc, kmeans_model_dir=None, kmeans_pca_model_dir=None, save_plots=True, save_dir=None, prefix='', scaler_dir=None, compute_stats=True):\n",
    "    if not do_kmeans:\n",
    "        print('K-Means PCA analysis is not allowed')\n",
    "        return\n",
    "    \n",
    "    if save_dir is None:\n",
    "        save_dir = kmeans_pca_model_dir\n",
    "    if scaler_dir is None:\n",
    "        scaler_dir = kmeans_pca_model_dir\n",
    "    _kmeans_model_prefix = train_config.name\n",
    "    _pca_model_prefix = \"{}.kmeans-pca\".format(train_config.name)\n",
    "    enc_kmeans = get_enc_kmeans(test_enc, 10, save=True, load=True, save_dir=kmeans_model_dir, save_file_prefix=_kmeans_model_prefix)\n",
    "    enc_pca = get_enc_pca(enc_kmeans, 10, save=True, load=True, save_dir=kmeans_pca_model_dir, save_file_prefix=_pca_model_prefix)\n",
    "\n",
    "    for _scale, _method in ([(None, None)] + list(itertools.product(['features', 'across'], ['minmax', 'standard']))):\n",
    "        print('\\n\\nK-Means PCA Analysis. Scale: {}. Method: {}\\n\\n'.format(_scale, _method))\n",
    "        if _scale is None:\n",
    "            _stats_file_prefix = '{}kmeans-pca.unscaled.stats'.format(prefix)\n",
    "            enc_scaled = enc_pca\n",
    "        else:\n",
    "            scaler_prefix = _kmeans_model_prefix\n",
    "            enc_scaled = get_enc_scaled(enc_pca, _scale, _method, save=True, load=True, save_dir=scaler_dir, prefix=scaler_prefix)\n",
    "            _stats_file_prefix = '{}kmeans-pca.scaled_{}.method_{}.stats'.format(prefix, _scale, _method)\n",
    "        if compute_stats:\n",
    "            print_enc_stats(enc_scaled, save_plots=save_plots, save_dir=save_dir, save_file_prefix=_stats_file_prefix)\n",
    "if True:\n",
    "    kmeans_pca_analysis(test_enc, kmeans_model_dir=os.path.join(analysis_dir, 'kmeans'),\n",
    "                       kmeans_pca_model_dir=os.path.join(analysis_dir, 'kmeans-pca'), compute_stats=compute_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENRE WISE ANALYSIS\n",
    "def genre_wise_analysis():\n",
    "    genres_map = commons.get_genres_map(dataset_config.datasets_dir, dataset_config.fma_type, reverse=True)\n",
    "    enc_genres = commons.map_indices_to_genre(enc_segs, dataset_config.fma_meta_dir, dataset_config.fma_type)\n",
    "    for genre in ['Electronic', 'Rock', 'Folk']:\n",
    "        print('ANALYSING GENRE: {}'.format(genre))\n",
    "        enc = np.array([e for i,e in filter(lambda enum_enc: enc_genres[enum_enc[0]] == genres_map[genre], enumerate(test_enc))])\n",
    "        raw_enc_analysis(enc, save_dir=os.path.join(analysis_dir, 'genre-wise', genre, 'raw'),\n",
    "                         prefix=\"{}_\".format(genre), scaler_dir=os.path.join(analysis_dir, 'raw'))\n",
    "        pca_analysis(enc, model_dir=os.path.join(analysis_dir, 'pca'),\n",
    "                     save_dir=os.path.join(analysis_dir, 'genre-wise', genre, 'pca'), prefix=\"{}_\".format(genre))\n",
    "        kmeans_analysis(enc, model_dir=os.path.join(analysis_dir, 'kmeans'),\n",
    "                     save_dir=os.path.join(analysis_dir, 'genre-wise', genre, 'kmeans'), prefix=\"{}_\".format(genre))\n",
    "        kmeans_pca_analysis(enc, kmeans_model_dir=os.path.join(analysis_dir, 'kmeans'),\n",
    "                            kmeans_pca_model_dir=os.path.join(analysis_dir, 'kmeans-pca'),\n",
    "                            save_dir=os.path.join(analysis_dir, 'genre-wise', genre, 'kmeans-pca'), prefix=\"{}_\".format(genre))\n",
    "if True and compute_stats:\n",
    "    genre_wise_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRACK WISE ANALYSIS\n",
    "def track_wise_analysis(num_tracks):\n",
    "    random_tracks = np.random.choice(np.unique(enc_segs), num_tracks)\n",
    "    genre_idx = commons.map_indices_to_genre(random_tracks, dataset_config.fma_meta_dir, dataset_config.fma_type)\n",
    "    genres_map = commons.get_genres_map(dataset_config.datasets_dir, dataset_config.fma_type, reverse=False)\n",
    "    for i, track_idx in enumerate(random_tracks):\n",
    "        genre = genres_map[genre_idx[i]]\n",
    "        prefix = \"{}_{}\".format(track_idx, genre)\n",
    "        print('ANALYSING TRACK: {} GENRE: {}'.format(track_idx, genre))\n",
    "        enc = np.array([e for i,e in filter(lambda enum_enc: enc_segs[enum_enc[0]] == track_idx, enumerate(test_enc))])\n",
    "        raw_enc_analysis(enc, save_dir=os.path.join(analysis_dir, 'track-wise', prefix, 'raw'),\n",
    "                         prefix=\"{}_\".format(prefix), scaler_dir=os.path.join(analysis_dir, 'raw'))\n",
    "        pca_analysis(enc, model_dir=os.path.join(analysis_dir, 'pca'),\n",
    "                     save_dir=os.path.join(analysis_dir, 'track-wise', prefix, 'pca'), prefix=\"{}_\".format(prefix))\n",
    "        kmeans_analysis(enc, model_dir=os.path.join(analysis_dir, 'kmeans'),\n",
    "                     save_dir=os.path.join(analysis_dir, 'track-wise', prefix, 'kmeans'), prefix=\"{}_\".format(prefix))\n",
    "        kmeans_pca_analysis(enc, kmeans_model_dir=os.path.join(analysis_dir, 'kmeans'),\n",
    "                            kmeans_pca_model_dir=os.path.join(analysis_dir, 'kmeans-pca'),\n",
    "                            save_dir=os.path.join(analysis_dir, 'track-wise', prefix, 'kmeans-pca'), prefix=\"{}_\".format(prefix))\n",
    "if False and compute_stats:\n",
    "    track_wise_analysis(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRACK WISE ANALYSIS - SPECIFIC TRACK\n",
    "def specific_track_analysis(track_idx):\n",
    "    enc = np.array([e for i,e in filter(lambda enum_enc: enc_segs[enum_enc[0]] == track_idx, enumerate(test_enc))])\n",
    "    raw_enc_analysis(enc, save_plots=False, scaler_dir=os.path.join(analysis_dir, 'raw'))\n",
    "    pca_analysis(enc, save_plots=False, model_dir=os.path.join(analysis_dir, 'pca'))\n",
    "    kmeans_analysis(enc, save_plots=False, model_dir=os.path.join(analysis_dir, 'kmeans'))\n",
    "    kmeans_pca_analysis(enc, save_plots=False, kmeans_model_dir=os.path.join(analysis_dir, 'kmeans'),\n",
    "                            kmeans_pca_model_dir=os.path.join(analysis_dir, 'kmeans-pca'))\n",
    "if False:\n",
    "    specific_track_analysis(125548)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion Music\n",
    "\n",
    "import IPython\n",
    "import fma_utils\n",
    "tracks = commons.get_fma_meta(\"datasets/fma/fma_metadata\", 'small')\n",
    "track_id = np.random.choice(tracks.index)\n",
    "track_path = fma_utils.get_audio_path('datasets/fma/fma_small', int(track_id))\n",
    "IPython.display.Audio(filename=track_path, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepviz",
   "language": "python",
   "name": "deepviz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
